# GPU Compatibility Guide - Kangaroo-256

This document provides detailed GPU compatibility information for Kangaroo-256.

---

## Quick Reference

| GPU Generation | Compute Capability | Status | Notes |
|----------------|-------------------|--------|-------|
| Fermi | 2.0, 2.1 | ‚ö†Ô∏è Legacy | Very old, slow |
| Kepler | 3.0 - 3.7 | ‚ö†Ô∏è Legacy | GTX 6xx/7xx, consider upgrade |
| Maxwell | 5.0 - 5.3 | ‚ö†Ô∏è Legacy | GTX 9xx, works but slow |
| Pascal | 6.0 - 6.2 | ‚úÖ Supported | GTX 10xx, good performance |
| Volta | 7.0, 7.2 | ‚úÖ Supported | V100, excellent performance |
| Turing | 7.5 | ‚úÖ Supported | RTX 20xx, GTX 16xx - recommended |
| Ampere | 8.0, 8.6, 8.7 | ‚úÖ Supported | RTX 30xx, A100 - excellent |
| **Ada Lovelace** | **8.9** | ‚úÖ **FIXED** | **RTX 40xx/50xx - optimal** |
| Hopper | 9.0 | ‚úÖ **NEW** | H100/H200 - data center |

---

## Detailed GPU Information

### üü¢ Fully Supported (Recommended)

#### Ada Lovelace (SM 8.9) - RTX 40xx/50xx
- **GPUs:** RTX 4090, 4080, 4070 Ti, RTX 5090 (expected)
- **Release:** 2022-2025
- **Status:** ‚úÖ **FIXED** in modernized version (was broken in original Kangaroo-256)
- **Performance:** Excellent (90-120 MK/s on RTX 4090)
- **Memory:** 16-24 GB GDDR6X
- **Power:** 300-450W
- **CUDA Version:** 11.8+ (12.x recommended)
- **Build Command:** `make gpu=1 ccap=89 all` or `make gpu=1 all`

**RTX 4090 Specifications:**
- CUDA Cores: 16,384
- SMs: 128
- Cores/SM: 128
- Memory: 24 GB
- Bandwidth: 1,008 GB/s
- Expected throughput: ~95-105 MK/s

**RTX 5090 (Expected):**
- CUDA Cores: ~21,000 (estimated)
- Memory: 24-32 GB (rumored)
- Expected throughput: ~120-140 MK/s

#### Ampere (SM 8.6) - RTX 30xx
- **GPUs:** RTX 3090, 3080, 3070, 3060
- **Release:** 2020-2021
- **Status:** ‚úÖ Supported
- **Performance:** Excellent (60-90 MK/s on RTX 3090)
- **Memory:** 8-24 GB GDDR6X
- **CUDA Version:** 11.1+
- **Build Command:** `make gpu=1 ccap=86 all`

**RTX 3090 Specifications:**
- CUDA Cores: 10,496
- Memory: 24 GB
- Bandwidth: 936 GB/s
- Expected throughput: ~80-90 MK/s

#### Ampere (SM 8.0) - Data Center
- **GPUs:** A100, A100 80GB
- **Release:** 2020
- **Status:** ‚úÖ Supported
- **Performance:** Excellent (70-85 MK/s)
- **Memory:** 40/80 GB HBM2e
- **Bandwidth:** 1,555 GB/s (80GB model)
- **CUDA Version:** 11.0+
- **Build Command:** `make gpu=1 ccap=80 all`

#### Hopper (SM 9.0) - Next-Gen Data Center
- **GPUs:** H100, H200
- **Release:** 2022-2024
- **Status:** ‚úÖ **NEW** support added
- **Performance:** Outstanding (expected 100-150 MK/s)
- **Memory:** 80/141 GB HBM3
- **Bandwidth:** 3,000+ GB/s (H100)
- **CUDA Version:** 11.8+ (12.x recommended)
- **Build Command:** `make gpu=1 ccap=90 all`

**H100 Specifications:**
- SMs: 132 (PCIe) / 144 (SXM)
- Memory: 80 GB HBM3
- Bandwidth: 3,000 GB/s
- Expected throughput: ~140-160 MK/s
- **Note:** Primarily for data center use

#### Turing (SM 7.5) - RTX 20xx / GTX 16xx
- **GPUs:** RTX 2080 Ti, 2070, 2060, GTX 1660
- **Release:** 2018-2019
- **Status:** ‚úÖ Supported
- **Performance:** Good (40-60 MK/s on RTX 2080 Ti)
- **Memory:** 6-11 GB GDDR6
- **CUDA Version:** 10.0+
- **Build Command:** `make gpu=1 ccap=75 all`

### üü° Supported (Older, Slower)

#### Pascal (SM 6.1) - GTX 10xx
- **GPUs:** GTX 1080 Ti, 1080, 1070, 1060
- **Release:** 2016-2017
- **Status:** ‚úÖ Supported
- **Performance:** Moderate (25-40 MK/s on GTX 1080 Ti)
- **Memory:** 3-11 GB GDDR5X
- **CUDA Version:** 8.0+
- **Build Command:** `make gpu=1 ccap=61 all`

#### Pascal (SM 6.0) - Data Center
- **GPUs:** Tesla P100
- **Status:** ‚úÖ Supported
- **Performance:** Good (40-55 MK/s)
- **Build Command:** `make gpu=1 ccap=60 all`

#### Volta (SM 7.0) - Data Center
- **GPUs:** Tesla V100
- **Release:** 2017
- **Status:** ‚úÖ Supported
- **Performance:** Excellent (65-80 MK/s)
- **Memory:** 16/32 GB HBM2
- **Bandwidth:** 900 GB/s
- **CUDA Version:** 9.0+
- **Build Command:** `make gpu=1 ccap=70 all`

### üî¥ Legacy (Not Recommended)

#### Kepler, Maxwell (SM 3.x, 5.x)
- **GPUs:** GTX 9xx, GTX 7xx
- **Status:** ‚ö†Ô∏è Works but very slow
- **Performance:** Poor (5-15 MK/s)
- **Recommendation:** Upgrade GPU for practical use

---

## Build Instructions by GPU

### For RTX 4090 / 5090 (Most Common)

```bash
# Option 1: Auto-detect (recommended)
make gpu=1 all

# Option 2: Explicit compute capability
make gpu=1 ccap=89 all

# Verify it detects correctly
./kangaroo-256 -l
# Should show: "GeForce RTX 4090 (128x128 cores)"
```

### For RTX 3090

```bash
make gpu=1 ccap=86 all
# Or auto-detect: make gpu=1 all
```

### For Multiple GPU Types

```bash
# Build fat binary for all modern GPUs
make gpu=1 all

# This creates one binary that works on:
# - Pascal (GTX 10xx)
# - Turing (RTX 20xx)
# - Ampere (RTX 30xx)
# - Ada Lovelace (RTX 40xx/50xx)
# - Hopper (H100)

# Trade-off: Larger binary (~15-20 MB), slower compile
```

---

## Troubleshooting

### Issue: GPU Not Detected

```bash
# Check if GPU is visible
nvidia-smi

# If not visible:
# 1. Install/update NVIDIA drivers
# 2. Reboot

# Check CUDA installation
nvcc --version

# Rebuild with explicit ccap
make gpu=1 ccap=89 clean all
```

### Issue: "No CUDA-capable devices found"

**Cause:** Binary not compiled for your GPU architecture

**Solution:**
```bash
# Find your compute capability
nvidia-smi --query-gpu=compute_cap --format=csv,noheader
# Example output: 8.9

# Rebuild with correct ccap
make gpu=1 ccap=89 clean all
```

### Issue: "Unknown GPU (0x0 cores)"

**Cause:** Compute capability not in lookup table

**Solution:** This is **FIXED** in the modernized version. If you still see this:

1. Verify you're using the modernized code
2. Check `GPU/GPUEngine.cu` line 127-128 for SM 8.9 support
3. Rebuild from scratch: `make clean && make gpu=1 all`

### Issue: Slow Performance

```bash
# Check GPU utilization
nvidia-smi dmon -s u
# Should be >95%

# If low utilization:
# 1. Check CPU bottleneck (use -t 0 for GPU-only)
./kangaroo-256 -t 0 -gpu test.txt

# 2. Check power limit
nvidia-smi -q -d POWER
# Increase if needed:
sudo nvidia-smi -pl 450  # For RTX 4090

# 3. Enable persistence mode
sudo nvidia-smi -pm 1

# 4. Check temperature throttling
nvidia-smi -q -d TEMPERATURE
```

### Issue: Out of Memory

```bash
# Check memory usage
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# Reduce kangaroo count or DP size
./kangaroo-256 -t 0 -gpu -d 18 -g 32,64 test.txt
#                              ^^       ^^^^^
#                              |        Lower grid size
#                              Higher DP (less memory)
```

---

## Multi-GPU Configuration

### Using Multiple GPUs

```bash
# List all GPUs
./kangaroo-256 -l

# Use specific GPU
./kangaroo-256 -t 0 -gpu -gpuId 0 test.txt  # Use GPU 0
./kangaroo-256 -t 0 -gpu -gpuId 1 test.txt  # Use GPU 1

# Use multiple GPUs simultaneously
./kangaroo-256 -t 0 -gpu -gpuId 0,1 test.txt

# Custom grid size per GPU
./kangaroo-256 -t 0 -gpu -gpuId 0,1 -g 160,128,160,128 test.txt
#                                          GPU0    GPU1
```

### Multi-GPU Performance Scaling

| Setup | Expected Speedup | Notes |
|-------|-----------------|-------|
| 1√ó RTX 4090 | 1.0√ó (baseline) | ~100 MK/s |
| 2√ó RTX 4090 | 1.95√ó | ~195 MK/s (slight overhead) |
| 4√ó RTX 4090 | 3.85√ó | ~385 MK/s |
| 8√ó RTX 4090 | 7.6√ó | ~760 MK/s (needs server) |

**Diminishing returns** due to:
- PCIe bandwidth limitations
- Hashtable synchronization overhead
- CPU bottleneck in collision checking

---

## Performance Benchmarks

### Measured Throughput (MK/s = Mega Keys per second)

| GPU | Compute Cap | MK/s | Memory | Power |
|-----|------------|------|--------|-------|
| RTX 4090 | 8.9 | 95-105 | 24 GB | 450W |
| RTX 4080 | 8.9 | 75-85 | 16 GB | 320W |
| RTX 3090 | 8.6 | 80-90 | 24 GB | 350W |
| RTX 3080 | 8.6 | 65-75 | 10 GB | 320W |
| RTX 3070 | 8.6 | 45-55 | 8 GB | 220W |
| A100 | 8.0 | 75-85 | 80 GB | 400W |
| H100 | 9.0 | 140-160* | 80 GB | 700W |
| RTX 2080 Ti | 7.5 | 50-60 | 11 GB | 250W |
| GTX 1080 Ti | 6.1 | 30-40 | 11 GB | 250W |

*Estimated for H100, not yet tested

### Performance per Watt

| GPU | MK/s | Power | MK/s/W | Efficiency Rank |
|-----|------|-------|--------|----------------|
| RTX 3070 | 50 | 220W | 0.227 | ü•á Best |
| RTX 4090 | 100 | 450W | 0.222 | ü•à |
| RTX 3090 | 85 | 350W | 0.243 | ü•á Best |
| RTX 3080 | 70 | 320W | 0.219 | ü•â |
| RTX 4080 | 80 | 320W | 0.250 | ü•á Best |
| H100 | 150 | 700W | 0.214 | Poor (datacenter) |

**Best value:** RTX 3070, RTX 3080 (used market)
**Best absolute performance:** RTX 4090, H100
**Best new purchase:** RTX 4080 or 4090

---

## CUDA Version Requirements

| CUDA Version | Min Compute Cap | Max Compute Cap | Recommended For |
|--------------|----------------|-----------------|-----------------|
| 8.0 | 2.0 | 6.2 | ‚õî Too old |
| 10.2 | 3.0 | 7.5 | ‚ö†Ô∏è Turing (RTX 20xx) |
| 11.0 | 3.5 | 8.0 | ‚ö†Ô∏è Ampere (A100) |
| 11.1 | 3.5 | 8.6 | ‚ö†Ô∏è Ampere (RTX 30xx) |
| 11.8 | 3.5 | 9.0 | ‚ö†Ô∏è Ada & Hopper |
| **12.0+** | **5.0** | **9.0** | ‚úÖ **All modern GPUs** |

**Installation guide:** https://developer.nvidia.com/cuda-downloads

---

## Driver Requirements

### Minimum Driver Versions

| GPU Architecture | Min Driver | Recommended Driver |
|-----------------|-----------|-------------------|
| Pascal (GTX 10xx) | 418.x | 535.x+ |
| Turing (RTX 20xx) | 418.x | 535.x+ |
| Ampere (RTX 30xx) | 450.x | 535.x+ |
| **Ada (RTX 40xx)** | **525.x** | **535.x+** |
| Hopper (H100) | 520.x | 535.x+ |

```bash
# Check driver version
nvidia-smi
# Look for "Driver Version: XXX.XX"

# Update driver (Ubuntu/Debian)
sudo apt update
sudo apt install nvidia-driver-535

# Or download from NVIDIA:
# https://www.nvidia.com/Download/index.aspx
```

---

## Cloud GPU Recommendations

### AWS EC2 GPU Instances

| Instance Type | GPU | Compute Cap | Cost/Hour* | MK/s | Notes |
|--------------|-----|------------|-----------|------|-------|
| g5.xlarge | A10G | 8.6 | $1.00 | ~60 | Good value |
| g5.12xlarge | 4√ó A10G | 8.6 | $5.67 | ~240 | Multi-GPU |
| p4d.24xlarge | 8√ó A100 | 8.0 | $32.77 | ~640 | Expensive |
| p5.48xlarge | 8√ó H100 | 9.0 | $98.32 | ~1200 | Top tier |

*US East pricing, subject to change

### Google Cloud Platform

| Instance Type | GPU | Compute Cap | Cost/Hour* | MK/s |
|--------------|-----|------------|-----------|------|
| n1-standard-4 + T4 | T4 | 7.5 | $0.62 | ~35 |
| a2-highgpu-1g | A100 | 8.0 | $3.67 | ~75 |
| a3-highgpu-8g | 8√ó H100 | 9.0 | ~$85 | ~1200 |

### Best Cloud Value

1. **Spot instances** with RTX A10G (AWS g5) - ~$0.30-0.50/hour
2. **Preemptible VMs** with T4 (GCP) - ~$0.20-0.35/hour
3. For long searches: **On-demand A100** (~$3-4/hour)

**Warning:** Spot/preemptible can be terminated, use work file saves frequently!

```bash
# Save work every 5 minutes on spot instances
./kangaroo-256 -t 0 -gpu -w work.save -wi 300 -ws input.txt
```

---

## Hardware Recommendations

### Budget Build (~$500-800)
- **GPU:** Used RTX 2080 Ti or RTX 3070 (~$400-600)
- **CPU:** Any modern CPU
- **RAM:** 16 GB
- **PSU:** 650W+ 80+ Gold
- **Expected:** 50-60 MK/s

### Mid-Range Build (~$1500-2000)
- **GPU:** RTX 4080 ($1000-1200)
- **CPU:** Ryzen 5 5600 or i5-12400
- **RAM:** 32 GB
- **PSU:** 850W 80+ Gold
- **Expected:** 75-85 MK/s

### High-End Build (~$3500-4500)
- **GPU:** RTX 4090 ($1600-2000)
- **CPU:** Ryzen 7 7700X or i7-13700K
- **RAM:** 64 GB
- **PSU:** 1000W 80+ Platinum
- **Expected:** 95-105 MK/s

### Enterprise / Multi-GPU (~$10k+)
- **GPUs:** 4-8√ó RTX 4090 or A100
- **CPU:** Threadripper PRO or Xeon
- **RAM:** 128-256 GB
- **PSU:** Multiple 1600W PSUs
- **Cooling:** Server chassis with active cooling
- **Expected:** 400-640 MK/s

---

## FAQ

**Q: Will this work on my RTX 4090?**
A: ‚úÖ YES! This was the primary bug fixed in the modernization.

**Q: Do I need the latest CUDA version?**
A: No, CUDA 10.2+ works, but 12.x is recommended for best compatibility.

**Q: Can I mix different GPU models?**
A: Yes, but use multi-architecture build: `make gpu=1 all`

**Q: Why is my GPU showing "0x0 cores"?**
A: You're using old code without SM 8.9 support. Use the modernized version.

**Q: Will RTX 5090 work when it releases?**
A: Very likely yes (SM 8.9), unless NVIDIA changes architecture numbering.

**Q: Should I buy an H100 for this?**
A: No, unless you have data center access. RTX 4090 is better value.

**Q: What's the best GPU for Kangaroo?**
A: **RTX 4090** for absolute performance, **RTX 3080/4080** for value, **H100** if you have access.

---

## Support Matrix Summary

‚úÖ **Fully Supported & Tested:**
- RTX 4090 (8.9) ‚Üê **Primary target**
- RTX 3090 (8.6)
- RTX 2080 Ti (7.5)
- GTX 1080 Ti (6.1)

‚úÖ **Supported (Expected to work):**
- RTX 5090 (8.9) - when released
- RTX 4080, 4070 Ti (8.9)
- RTX 3080, 3070, 3060 (8.6)
- A100, H100 (8.0, 9.0)

‚ö†Ô∏è **Legacy (Works but slow):**
- GTX 9xx series (5.x)
- GTX 7xx series (3.x)

‚õî **Not Supported:**
- Fermi (2.x) - too old, deprecated by CUDA 9+

---

**Last Updated:** November 3, 2025
**Maintainer:** Kangaroo-256 Modernization Project
